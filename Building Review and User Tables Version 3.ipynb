{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ba3dc88-d039-42e4-a0e4-318ca273d3cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Building Review and User Tables\n",
    "The review and user data files are the largest files you will be using in class and in your project.  Loading a DataFrame, selecting the needed columns, and caching the data from the JSON for either data file is slow, so this notebook instead loads the data and creates Spark tables named `reviews_without_text_table` and `user_table` based on the Parquet format that's common in Big Data.  Going forward, the tables can be used (on that same running cluster) the same as a temporary view. On a subsequently created cluster, the table definition will be gone, but the Parquet files used to build the tables will still exist and this notebook will rebuild them in less than a minute. \n",
    "\n",
    "Since the original code below excludes some of the fields in each set\n",
    "of JSON data files when building the tables initially, if you have already run this notebook and created the tables, but then want to include a different set of fields for either table,\n",
    "rerun the cell that builds that table, but ***temporarily*** set the corresponding variable to force the rebuilding of the table from the JSON data files to True.  Be sure after the table is rebuilt to set it back to False.\n",
    "\n",
    "<span style=\"color:red;\">Be sure to run this notebook before we cover SQL or you use the SQL Examples notebook</span>\n",
    "\n",
    "The first time this notebook is run, it will take up to 45 minutes to build both tables from the JSON files.  To rerun if you come back to your account on a new cluster, it should take less than a minute.\n",
    "\n",
    "As noted above, once the tables are built, they can be used in Spark SQL queries the same as temporary views in Spark SQL queries.\n",
    "\n",
    "**NOTE:** The following cell includes imports and function definitions that are needed when running the cells further down for building the `reviews_without_text_table` and `user_table`, so the next cell always needs to be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2326cb8c-de82-4a07-b6c1-f5c6eb6b31f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "def directory_exists(path):\n",
    "  '''Checks if a directory exists and contains some files.\n",
    "     If the directory exists, but contains no files,\n",
    "     then treated the same as if the directory did not exist.\n",
    "     Returns True if the directory exists and contains files, \n",
    "     otherwise False\n",
    "  '''\n",
    "  try:\n",
    "    file_list = dbutils.fs.ls(path)\n",
    "    if len(file_list) > 0:\n",
    "      return(True)\n",
    "    else:\n",
    "      return(False)\n",
    "  except Exception:\n",
    "    return(False) #if empty or path does not exist\n",
    "  \n",
    "\n",
    "def file_exists(path):\n",
    "  '''Checks if the path provided is a file.\n",
    "     If the path exists, the list function \n",
    "     will contain a list with one element.\n",
    "     Returns True if the file exists and contains files, \n",
    "     otherwise False\n",
    "  '''\n",
    "  try:\n",
    "    file_list = dbutils.fs.ls(path)\n",
    "    if len(file_list) > 0:\n",
    "      return(True)\n",
    "    else:\n",
    "      return(False)\n",
    "  except Exception:\n",
    "    return(False) #if path does not exist\n",
    "\n",
    "\n",
    "def build_table(table_path, table_name):\n",
    "  '''Given the path to where the files for the table are, which is \n",
    "     generally under /user/hive/warehouse, and the name of the table,\n",
    "     this function builds the table.  Note that it assumes the table\n",
    "     was originally built using PARQUET.\n",
    "  '''\n",
    "  print(f\"building {table_name} from existing table files\")\n",
    "  spark.sql(f\"\"\"\n",
    "    CREATE TABLE {table_name} \n",
    "    USING PARQUET \n",
    "    LOCATION '{table_path}' \n",
    "  \"\"\")\n",
    "    \n",
    "\n",
    "def table_summary(field_name, table_name):\n",
    "  '''Given a table name and a field to count in the table, \n",
    "     this function generates a count of the number of records \n",
    "     and shows the first 10 rows truncated at 22 characters. \n",
    "  '''\n",
    "  spark.sql(f\"\"\"\n",
    "    SELECT COUNT({field_name}) AS record_count\n",
    "    FROM {table_name}\n",
    "  \"\"\").show()\n",
    "  # show 10 rows from the table\n",
    "  spark.sql(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM {table_name} LIMIT 10 \n",
    "  \"\"\").show(truncate=22)\n",
    "\n",
    "\n",
    "def process_table(table_name, table_path, data_path, field_name, force_rebuild, json_read_function,summary=True):\n",
    "  '''This routine is called to create the tables based on the review or user data\n",
    "     The 7 parameters passed are:\n",
    "     1. table_name: string with the name of the table being created\n",
    "     2. table_path: string with the path in DBFS to where the table files in a Parquet format are located.  Usually under /user/hive/warehouse/\n",
    "     3. data_path: string with the DBFS path of the compressed JSON data file\n",
    "     4. field_name: string with the name of a field within the resulting table that will always be included and not Null\n",
    "     5. force_rebuild: a Boolean value as to whether the table should be rebuilt from the JSON files even if the table files exist\n",
    "     6. json_read_function: the name of a function that takes one parameter, the path to the JSON data, and returns the cached DataFrame that was created\n",
    "     7. summary: optional boolean that specifies if a summary of the created table should be shown. Default is True\n",
    "  '''\n",
    "  if ( spark.catalog._jcatalog.tableExists(table_name) and force_rebuild == False):\n",
    "    print(f\"{table_name} table exists\")\n",
    "  else:\n",
    "    # If the table files exist and the download is NOT being forced, build from the existing table files\n",
    "    if directory_exists(table_path) and force_rebuild == False:\n",
    "      build_table(table_path, table_name)\n",
    "    else: # create dataframe from JSON files and save as a table\n",
    "      # delete existing table files if they exist\n",
    "      try:\n",
    "        dbutils.fs.rm(table_path, recurse=True)\n",
    "      except Exception:\n",
    "        pass #if the directory did not exist, then the files already were gone or never existed\n",
    "      # Check that the JSON data file exists\n",
    "      if file_exists(data_path) == False:\n",
    "        raise Exception(f\"The path to the compressed JSON data file: {data_path} does not exist.\")\n",
    "      print(f\"building {table_name} from JSON file\")\n",
    "      df_temp = json_read_function(data_path)\n",
    "      # Does not already exist as a table, so write it out\n",
    "      df_temp.write.mode(\"overwrite\").format('parquet').saveAsTable(table_name)\n",
    "      df_temp.unpersist()\n",
    "  # Regardless of whether table was built from existing PARQUET files or JSON files, \n",
    "  # show some summary information for the table if the summary parameter is True\n",
    "  if summary:\n",
    "    table_summary(field_name, table_name)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5e53234-0b22-4714-8e7a-16a543db797d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Build the table for the review data\n",
    "The following cell uses the functions defined in the cell above, so that must be run first.  \n",
    "\n",
    "The code can be used in other projects to build the review table.  If different fields are desired in the table, \n",
    "then the `create_review_dataframe` function should be edited, but if that function is changed, and the table already exists, then \n",
    "the `FORCE_REVIEW_REBUILD` constant should ***temporarily*** be set to True.  When building the table from the JSON files, it\n",
    "will require pproximately 25 minutes, but when rebuilding the table from the table files later (on a new cluster), it will \n",
    "take less than 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ffe5672-ea7f-42ac-8986-92ebf7717013",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews_without_text_table table exists\n+------------+\n|record_count|\n+------------+\n|     6990280|\n+------------+\n\n+----------------------+----------------------+----------------------+----+-----+------+-------------------+-----+\n|             review_id|           business_id|               user_id|cool|funny|useful|               date|stars|\n+----------------------+----------------------+----------------------+----+-----+------+-------------------+-----+\n|6jtUDueEHRG2rbAJ75bTLA|dJPTV6ilP1FhayMIyVMN2A|e-lnkltoLKcVtXvipOFIDg|   0|    0|     0|2020-01-03 00:22:30|  3.0|\n|O-ZTyisaO1fRIyfadT3ssA|h8h1QvEQruTkBI50NlWsyA|Hco0gs1-5c07rHCDgxpfNg|   0|    0|     0|2020-01-31 12:10:05|  5.0|\n|Q6aYY1c0ENTkm-yvuZr-1w|36NolK-R-VXqKMh7DiNSWw|8LFk1qnvM3SmxtkCh8UkGQ|   0|    0|     1|2019-07-29 23:52:36|  5.0|\n|3j2US9m_3GyEcbYRzzc4gA|kOOiEcy0UaWLwCPgZ95nmg|iPkdNziqnhNukjnmsrSzmg|   1|    1|     7|2010-06-19 12:52:48|  2.0|\n|KS9COVn1Fq2TKYsofgU3AA|8gY3LGFUMIQubovGbSd7Nw|JGJ_Vx1GK4j80-mVWw7iSw|   2|    1|     3|2018-03-02 22:42:56|  5.0|\n|ECgAESU45-quv1xEdbn33Q|JEhPHeoLQhkOIWMZn6k8bw|kmo2-gxJkZjsTDOMXONjRQ|   4|    3|     5|2015-03-28 00:15:50|  5.0|\n|JBRRRqr7Vvn1NAyp9gl7wA|YVYHiXnOU5qLzx_3_tmFqw|ib0Y5bA2mP6UtWQEetnaFg|   0|    0|     8|2019-04-13 18:04:49|  1.0|\n|AUBD0p1gQihBGYgw4vCCLw|Gf6VxoCCxdbJkInNGH6l2A|B6KNw9Jm5_jlMG08ZX1JeQ|   0|    0|     1|2020-02-15 14:54:02|  5.0|\n|dKxu8k0TA2_EDNwbEiFt-g|RuPk-ScXo4ycpdnAkpfoNg|6knga-vNLcCxdQmhevWjTA|   2|    1|     4|2019-04-02 00:11:03|  4.0|\n|IKYZ7XWrN9VZxnF1QRmMYQ|y7EeYnpMJsgeu9HtqL9lqQ|UfwuBZ-_ltakHPBJAzfeKw|   0|    0|     7|2014-03-02 23:23:05|  3.0|\n+----------------------+----------------------+----------------------+----+-----+------+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# If you want to force the building of the table from the JSON files, set FORCE_REVIEW_REBUILD to True.\n",
    "# Only do this if records are missing from the table, it's not able to create the table, or you want to change the fields \n",
    "# included in the table since it is time consuming.\n",
    "# ************************************************************************************************\n",
    "# AFTER RUNNING IT TO REBUILD FROM THE JSON DATA BE SURE THAT FORCE_REVIEW_REBUILD IS SET TO FALSE\n",
    "# ************************************************************************************************\n",
    "FORCE_REVIEW_REBUILD = False \n",
    "REVIEW_DATA_PATH = \"/yelp/review.bz2\"\n",
    "REVIEW_TBL_PATH = \"/user/hive/warehouse/reviews_without_text_table\"\n",
    "REVIEW_TBL_NAME = \"reviews_without_text_table\"\n",
    "REVIEW_FIELD_NAME = \"business_id\" # field from the review table that's always included and never Null\n",
    "\n",
    "\n",
    "def create_review_dataframe(data_path):\n",
    "  df_reviews = spark.read.json(REVIEW_DATA_PATH).\\\n",
    "  select(\"review_id\",\"business_id\",\"user_id\",\"cool\",\"funny\",\"useful\",\"date\",\"stars\").cache()\n",
    "  return(df_reviews)\n",
    "  \n",
    "\n",
    "process_table(REVIEW_TBL_NAME, REVIEW_TBL_PATH, REVIEW_DATA_PATH, REVIEW_FIELD_NAME, FORCE_REVIEW_REBUILD, create_review_dataframe, summary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d03e776-db5e-4349-a068-f0cfc4d1ecc3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Build the table for the user data\n",
    "The following cell uses the functions defined in the first code cell above, so that must be run first.  \n",
    "\n",
    "The code can be used in other projects to build the user table.  If different fields are desired in the table, \n",
    "then the `create_user_dataframe` function should be edited, but if that function is changed, and the table already exists, then \n",
    "the `FORCE_USER_REBUILD` constant should ***temporarily*** be set to True.  When building the table from the JSON files, it\n",
    "will require pproximately 18 minutes, but when rebuilding the table from the table files later (on a new cluster), it will \n",
    "take less than 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ffd4952-e844-431f-ad94-fea9520fa89f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building user_table from JSON file\n+------------+\n|record_count|\n+------------+\n|     1987897|\n+------------+\n\n+-------------+---------------+---------------+----------------+--------------+---------------+---------------+---------------+-----------------+----------------+------------------+-----------------+----+----------------------+----+-----+-------------+------------+------+----------------------+-------------------+------------+\n|average_stars|compliment_cool|compliment_cute|compliment_funny|compliment_hot|compliment_list|compliment_more|compliment_note|compliment_photos|compliment_plain|compliment_profile|compliment_writer|cool|                 elite|fans|funny|         name|review_count|useful|               user_id|      yelping_since|friend_count|\n+-------------+---------------+---------------+----------------+--------------+---------------+---------------+---------------+-----------------+----------------+------------------+-----------------+----+----------------------+----+-----+-------------+------------+------+----------------------+-------------------+------------+\n|          5.0|              0|              0|               0|             0|              0|              0|              0|                0|               0|                 0|                0|   0|                      |   0|    1|        Avery|           5|     2|Rx4Jl0SK0Vu5xlw3RPbdmA|2016-11-16 20:58:16|         142|\n|          2.6|              1|              0|               1|             0|              0|              2|              0|                0|               0|                 0|                1|  11|                      |   2|    4|        Tanya|          42|    27|qL2n1KDDQccYD-Lx4MWmVw|2009-01-09 00:14:29|         123|\n|          4.0|              0|              0|               0|             0|              0|              0|              0|                0|               0|                 0|                0|   0|                      |   0|    0|      Abindas|           7|     2|P5_E3H8dWyTr4lJiaNUfTw|2015-08-17 00:10:07|         251|\n|         1.63|              0|              0|               0|             0|              0|              0|              0|                0|               0|                 0|                0|   1|                      |   0|    1| Zia-Kashanti|           7|     5|EF8JYiN8ohkAA8cipaRUpA|2010-05-27 12:23:58|          46|\n|          3.2|              4|              2|               4|             0|              0|              1|              1|                8|               7|                 0|                1|  22|                      |   9|   22|         Rick|          42|    71|lxpavrqhbfOvxIepv41RdQ|2011-09-30 19:36:53|           6|\n|         3.59|             30|              3|              30|             4|              0|              0|             14|                4|              16|                 1|                9| 567|2017,2018,2019,20,2...|  19|  266|       Margie|         334|   482|P_2DQxHwueC4Jm2qM-4Uuw|2017-06-02 00:17:41|         296|\n|          4.0|              0|              0|               0|             0|              0|              0|              0|                0|               0|                 0|                0|   0|                      |   0|    0|        Tyler|           1|     0|3dEhqPG8dMNQkSpz472JMA|2014-05-25 15:25:32|           2|\n|         1.33|              0|              0|               0|             0|              0|              0|              0|                0|               0|                 0|                0|   0|                      |   0|    3|         Doug|           9|     2|5qK2shRKiP9PGCKUPWbEaQ|2014-03-04 20:28:59|          57|\n|         4.38|              4|              0|               4|             0|              0|              0|              0|                0|               1|                 0|                1|  34|                      |   2|   17|Arnie Beauyet|          80|    53|_rS4QzfgKkMhwBDk9K2GvA|2012-06-06 18:18:34|          27|\n|          5.0|              0|              0|               0|             1|              0|              0|              0|                0|               0|                 0|                0|   9|                      |   0|    8|         Ryan|          18|    11|XKNe7pMsiHzdM6f8HUHFlw|2012-05-02 20:49:14|           3|\n+-------------+---------------+---------------+----------------+--------------+---------------+---------------+---------------+-----------------+----------------+------------------+-----------------+----+----------------------+----+-----+-------------+------------+------+----------------------+-------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#If you want to force the building of the table from the JSON files, set FORCE_USER_REBUILD to True.\n",
    "# Only do this if records are missing from the table, it's not able to create the table, or you want to change the fields \n",
    "# included in the table since it is time consuming.\n",
    "# **********************************************************************************************\n",
    "# AFTER RUNNING IT TO REBUILD FROM THE JSON DATA BE SURE THAT FORCE_USER_REBUILD IS SET TO FALSE\n",
    "# **********************************************************************************************\n",
    "FORCE_USER_REBUILD = False \n",
    "USER_DATA_PATH = \"/yelp/user.bz2\"\n",
    "USER_TBL_PATH = \"/user/hive/warehouse/user_table\"\n",
    "USER_TBL_NAME = \"user_table\"\n",
    "USER_FIELD_NAME = \"user_id\" # field from the user table that's always included and never Null\n",
    "\n",
    "\n",
    "def create_user_dataframe(data_path):\n",
    "  df_users = spark.read.json(USER_DATA_PATH).\\\n",
    "  withColumn(\"friend_count\",f.size(f.split(f.col(\"friends\"),'\\s*,\\s*') ) ).drop(\"friends\")\n",
    "  # withColumnRenamed(\"elite\",\"original_elite\").\\\n",
    "  # withColumn(\"elite\", f.regexp_replace( f.col(\"original_elite\"), '20,20','2020').alias(\"elite\") ).\\\n",
    "  # drop(\"original_elite\").cache()\n",
    "  return(df_users)\n",
    "  \n",
    "\n",
    "process_table(USER_TBL_NAME, USER_TBL_PATH, USER_DATA_PATH, USER_FIELD_NAME, FORCE_USER_REBUILD, create_user_dataframe, summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "865aba79-dec1-4b02-b462-acea6e23fbc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Building Review and User Tables Version 3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
