{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "570e20c5-a335-4c30-b881-ca465bdd78d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Functions used to create (and recreate) our tables\n",
    "The following cell contains some functions we will be using to write out our DataFrame as a table and also to recreate that table later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8e30689-acd8-4406-a0ae-582a98da6738",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "PATH_PREFIX = \"/user/hive/warehouse/\"\n",
    "\n",
    "def clear_table(table_name, table_path):\n",
    "  '''If the table exists then drop it.\n",
    "     If the table files exist, delete them.\n",
    "  '''\n",
    "  spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "  try:\n",
    "    dbutils.fs.rm(table_path, recurse=True)\n",
    "  except Exception:\n",
    "    pass #if the directory did not exist, then the files already were gone \n",
    "\n",
    "def check_dataframe_name(dataframe_name):\n",
    "  '''If given a variable name, this function determines if the \n",
    "     the name passed is an actual valid variable name for a DataFrame,\n",
    "     and if it is, returns that DataFrame.  If the variable does \n",
    "     not exist or is not a DataFrame, it returns None.  This is used so\n",
    "     we can use the same code to create or recreate a table, but if the \n",
    "     table files exist,we don't need to run the code to create the \n",
    "     DataFrame it's based on.\n",
    "  '''\n",
    "  if (dataframe_name is None):\n",
    "    df = None\n",
    "  elif (dataframe_name in locals() ):\n",
    "    df = locals()[dataframe_name]\n",
    "  elif (dataframe_name in globals() ):\n",
    "    df = globals()[dataframe_name]\n",
    "  else:\n",
    "    df = None\n",
    "  if df is not None and isinstance(df,DataFrame):\n",
    "    return(df)\n",
    "  else:\n",
    "    return(None)\n",
    "\n",
    "def files_exist(path):\n",
    "  '''If the directory path passed as a parameter exists and is a non-empty \n",
    "     directory (the directory contains files), this function returns true.\n",
    "  '''\n",
    "  files_exist = False # default, the directory does not exist or is empty\n",
    "  try:\n",
    "    file_list = dbutils.fs.ls(path)\n",
    "    if len(file_list) > 0:\n",
    "      return(True)\n",
    "  except Exception:\n",
    "    pass # files-exist is still False\n",
    "  return(False) #if empty or path does not exist\n",
    "\n",
    "def build_table(table_path, table_name):\n",
    "  '''Given the path to where the files for the table are, which is \n",
    "     generally under /user/hive/warehouse, and the name of the table,\n",
    "     this function builds the table.  Note that it assumes the table\n",
    "     was originally built using PARQUET.\n",
    "  '''\n",
    "  print(f\"building {table_name} from existing table files\")\n",
    "  spark.sql(f\"\"\"\n",
    "    CREATE TABLE {table_name} \n",
    "    USING PARQUET \n",
    "    LOCATION '{table_path}' \n",
    "  \"\"\")\n",
    "\n",
    "def table_summary(table_name):\n",
    "  '''Given a table name, this function will print the\n",
    "     number of records in the table and then print 10\n",
    "     records from the table.\n",
    "  '''\n",
    "  spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) AS record_count\n",
    "    FROM {table_name}\n",
    "  \"\"\").show()\n",
    "  # show 10 rows from the table\n",
    "  spark.sql(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM {table_name} LIMIT 10 \n",
    "  \"\"\").show(truncate=22)\n",
    "\n",
    "def process_or_create_table(table_name, dataframe_name=None, summary=False, delete=False):\n",
    "  '''This method is passed a table name and a DataFrame name.  If a table by that name \n",
    "     already exists, there is nothing to create.  If the files exist in the hive warehouse, \n",
    "     then the table is recreated from those files.  If the files don't exist, but the name \n",
    "     passed for the DataFrame name is an actual DataFrame, then the DataFrame is saved out as a table.\n",
    "     If the table fiels don't exist and the name passed as a parameter is not an existing DataFrame, \n",
    "     then an error message is printed. If the table exists or could be created, and summary is True, \n",
    "     then a record count and 10 rows are printed.\n",
    "  '''\n",
    "  table_path = PATH_PREFIX + table_name\n",
    "  if(delete):\n",
    "    clear_table(table_name, table_path)\n",
    "  df = check_dataframe_name(dataframe_name)\n",
    "  if ( spark.catalog._jcatalog.tableExists(table_name) ):\n",
    "    print(f\"{table_name} table exists\")\n",
    "  elif files_exist(table_path): # If the table files exist build from the existing table files\n",
    "    build_table(table_path, table_name)\n",
    "  elif df is not None: # save the specified DataFrame as a table using the Parquet file format\n",
    "    print(f\"Saving the {dataframe_name} DataFrame as table: {table_name}\")\n",
    "    df.write.mode(\"overwrite\").format('parquet').saveAsTable(table_name)\n",
    "  else:\n",
    "    print(\"The table files do not exist and no DataFrame was specified to be saved as a table, so the table was not created.\")\n",
    "    return\n",
    "  # show some summary information for the table if a field name was specified\n",
    "  if(summary):\n",
    "    table_summary(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71f10f88-9cf9-470b-9892-fa075ea19a3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create Table\n",
    "In the notebook that is generating the DataFrame that is written to a table, we can add a function call to the `process_or_create_table` function defined in this notebook to either rebuild the table definition from the saved Parquet files or rebuild the table from the DataFrame usedto generate the table. Following is the sytax for this method call:\n",
    "\n",
    "`process_or_create_table(TBL_NAME, DF_NAME, summary=True, delete=False)`\n",
    "\n",
    "**Both the name of the table and the name of the DataFrame ust be passed as strings.**\n",
    "\n",
    "**If we are calling this method from a notebook that does not contain the DataFrame used to write the table (but we want to rebuild the table from the Parquet files), then the DataFrame name should be omitted or passed as `None`.**\n",
    "\n",
    "If we want to see a record count and show the first few rows, `summary` should be True.\n",
    "\n",
    "If we want to force the table to be recreated from the DataFrame, then `delete` should be True, but be sure to set it back to False after the table is recreated. \n",
    "\n",
    "**NOTE:** Since we want to be able to come back to this notebook and run this cell without creating the DataFrame variable, \n",
    "we need to pass the name of the DataFrame as a string so we can determine if it's a valid variable.  If not, then it will\n",
    "be treated the same as if we did not specify a DataFrame.  This allows us to come back to this notebook and just run the \n",
    "function definitions above and then the following cell without first running the code to create `df_bus_reviews`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f5d5e19-fc4a-44f8-a6bb-2cefb2695d4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TBL_NAME = \"business_reviews_table\"\n",
    "# DF_NAME = \"df_bus_reviews\"\n",
    "# process_or_create_table(TBL_NAME, DF_NAME, summary=True, delete=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Process or Create Table",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
